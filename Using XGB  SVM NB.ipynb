
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.max_colwidth=200\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31962, 3)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('train_E6oV3lV.csv')\n",
    "print train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>bihday your majesty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>#model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>factsguide: society now    #motivation</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label  \\\n",
       "0   1      0   \n",
       "1   2      0   \n",
       "2   3      0   \n",
       "3   4      0   \n",
       "4   5      0   \n",
       "\n",
       "                                                                                                                        tweet  \n",
       "0                       @user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction.   #run  \n",
       "1  @user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx.    #disapointed #getthanked  \n",
       "2                                                                                                         bihday your majesty  \n",
       "3                                      #model   i love u take with u all the time in urð±!!! ðððð",
       "ð¦ð¦ð¦    \n",
       "4                                                                                      factsguide: society now    #motivation  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>@user #cnn calls #michigan middle school 'build the wall' chant '' #tcot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>retweet if you agree!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>@user @user lumpy says i am a . prove it lumpy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>35</td>\n",
       "      <td>1</td>\n",
       "      <td>it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    id  label  \\\n",
       "13  14      1   \n",
       "14  15      1   \n",
       "17  18      1   \n",
       "23  24      1   \n",
       "34  35      1   \n",
       "\n",
       "                                                                                                       tweet  \n",
       "13                                @user #cnn calls #michigan middle school 'build the wall' chant '' #tcot    \n",
       "14     no comment!  in #australia   #opkillingbay #seashepherd #helpcovedolphins #thecove  #helpcovedolphins  \n",
       "17                                                                                    retweet if you agree!   \n",
       "23                                                           @user @user lumpy says i am a . prove it lumpy.  \n",
       "34  it's unbelievable that in the 21st century we'd need something like this. again. #neverump  #xenophobia   "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train[train.label==1].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    29720\n",
      "1     2242\n",
      "dtype: int64\n",
      "% of hate tweets among all tweets 7.01457981353\n"
     ]
    }
   ],
   "source": [
    "## Postive VS Negative Tweets\n",
    "print train.label.value_counts()\n",
    "print '% of hate tweets among all tweets',100*2242.0/train.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 2)\n",
      "(9589, 2)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "x_train, x_test, y_train, y_test = train_test_split(train[\"tweet\"], train[\"label\"], test_size=0.3, random_state=4242)\n",
    "\n",
    "train_set = pd.concat([x_train,y_train],axis=1)\n",
    "test_set = pd.concat([x_test,y_test],axis=1)\n",
    "train_set.columns = [\"tweet\",\"label\"]\n",
    "test_set.columns = [\"tweet\",\"label\"]\n",
    "print train_set.shape\n",
    "print test_set.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))\n",
    "\n",
    "remove_words = ['user',\"frm\", \"u\", \"urs\", \"n\", \"ur\", \"b\", \"mro\", \"mo\", \"tmr\", \"k\", \"ok\",\n",
    "\"lol\", \"haha\", \"w\", \"moro\", \"yah\", \"cya\", \"cu\", \"eh\", \"hm\", \"hmm\",\n",
    "\"yall\", \"xoxo\", \"yolo\", \"em\", \"v\", \"ver\", \"hav\", \"vry\", \"shud\", \"wer\",\n",
    "\"abt\", \"bc\", \"wen\", \"jus\", \"tht\", \"fr\", \"hs\", \"r\", \"wud\", \"cud\"]\n",
    "\n",
    "def PreProcess(tweet): \n",
    "    #tweet = str(tweet)\n",
    "    tweet = tweet.lower() ## Convert the text to lower case\n",
    "    tweet = BeautifulSoup(tweet, \"lxml\").get_text() ## extract only text by dropping URLS\n",
    "    tweet=re.sub('[^0-9a-zA-Z]', ' ', tweet) ## extract only alpha numeric from a text\n",
    "    #tweet = re.sub(\"[^a-zA-Z]\", \" \", tweet)\n",
    "    tweet = ' '.join([word.strip() for word in tweet.split() if word not in stops]) ## Remove eng stop words\n",
    "    tweet = ' '.join([word for word in tweet.split() if word not in remove_words]) ## Remove specific words\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_set['cleaned_tweet'] = train_set.tweet.apply(PreProcess)\n",
    "test_set['cleaned_tweet'] = test_set.tweet.apply(PreProcess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31471</th>\n",
       "      <td>just watched this with my son he saw it for the first time #ripbillybob #restinpeace #actors  #movies #tosoon</td>\n",
       "      <td>0</td>\n",
       "      <td>watched son saw first time ripbillybob restinpeace actors movies tosoon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21663</th>\n",
       "      <td>just want to say i miss my little boy king every day but today i was missing one  son today   #king</td>\n",
       "      <td>0</td>\n",
       "      <td>want say miss little boy king every day today missing one son today king</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11277</th>\n",
       "      <td>when you are a pa of this incredible new company before it even launches!! pm me for details! ð    #maelle</td>\n",
       "      <td>0</td>\n",
       "      <td>pa incredible new company even launches pm details maelle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23305</th>\n",
       "      <td>smooth new year's eve! marijuana unleashed at ibooks.</td>\n",
       "      <td>1</td>\n",
       "      <td>smooth new year eve marijuana unleashed ibooks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17551</th>\n",
       "      <td>hardcore music nude asian women free</td>\n",
       "      <td>0</td>\n",
       "      <td>hardcore music nude asian women free</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                tweet  \\\n",
       "31471  just watched this with my son he saw it for the first time #ripbillybob #restinpeace #actors  #movies #tosoon    \n",
       "21663             just want to say i miss my little boy king every day but today i was missing one  son today   #king   \n",
       "11277  when you are a pa of this incredible new company before it even launches!! pm me for details! ð    #maelle    \n",
       "23305                                                        smooth new year's eve! marijuana unleashed at ibooks.      \n",
       "17551                                                                          hardcore music nude asian women free     \n",
       "\n",
       "       label  \\\n",
       "31471      0   \n",
       "21663      0   \n",
       "11277      0   \n",
       "23305      1   \n",
       "17551      0   \n",
       "\n",
       "                                                                  cleaned_tweet  \n",
       "31471   watched son saw first time ripbillybob restinpeace actors movies tosoon  \n",
       "21663  want say miss little boy king every day today missing one son today king  \n",
       "11277                 pa incredible new company even launches pm details maelle  \n",
       "23305                            smooth new year eve marijuana unleashed ibooks  \n",
       "17551                                      hardcore music nude asian women free  "
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Building TFIDF features### \n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 2),analyzer='word')\n",
    "\n",
    "\n",
    "train_tfidf = vectorizer.fit_transform(train_set.cleaned_tweet)\n",
    "\n",
    "test_tfidf = vectorizer.transform(test_set.cleaned_tweet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#help(TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22373, 144365)\n",
      "(9589, 144365)\n"
     ]
    }
   ],
   "source": [
    "#train_tfidf = train_tfidf[4::]\n",
    "print train_tfidf.shape\n",
    "print test_tfidf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "n_comp = 10\n",
    "svd_obj = TruncatedSVD(n_components=n_comp, algorithm='arpack')\n",
    "svd_obj.fit(train_tfidf)\n",
    "train_svd = pd.DataFrame(svd_obj.transform(train_tfidf))\n",
    "test_svd = pd.DataFrame(svd_obj.transform(test_tfidf))\n",
    "    \n",
    "train_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]\n",
    "test_svd.columns = ['svd_word_'+str(i) for i in range(n_comp)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[0]\tcv-test-error:0.068929+0.001159\tcv-train-error:0.068504+0.001118\n",
      "[1]\tcv-test-error:0.069152+0.001479\tcv-train-error:0.069107+0.000798\n",
      "[2]\tcv-test-error:0.068482+0.001283\tcv-train-error:0.068481+0.001174\n",
      "[3]\tcv-test-error:0.068705+0.001596\tcv-train-error:0.068548+0.001056\n",
      "[4]\tcv-test-error:0.068705+0.001596\tcv-train-error:0.068571+0.001140\n",
      "[5]\tcv-test-error:0.068258+0.001505\tcv-train-error:0.068236+0.000829\n",
      "[6]\tcv-test-error:0.068214+0.001554\tcv-train-error:0.068236+0.000779\n",
      "[7]\tcv-test-error:0.068571+0.001972\tcv-train-error:0.068370+0.000259\n",
      "[8]\tcv-test-error:0.068526+0.001829\tcv-train-error:0.068415+0.000662\n",
      "[9]\tcv-test-error:0.068615+0.001917\tcv-train-error:0.068392+0.000289\n",
      "[10]\tcv-test-error:0.068258+0.002046\tcv-train-error:0.067945+0.000669\n",
      "[11]\tcv-test-error:0.068347+0.001987\tcv-train-error:0.068169+0.000439\n",
      "[12]\tcv-test-error:0.067945+0.001644\tcv-train-error:0.067923+0.000522\n",
      "[13]\tcv-test-error:0.067856+0.001653\tcv-train-error:0.067699+0.000743\n",
      "[14]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067587+0.000776\n",
      "[15]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067520+0.000684\n",
      "[16]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067520+0.000684\n",
      "[17]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067520+0.000684\n",
      "[18]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067476+0.000704\n",
      "[19]\tcv-test-error:0.067632+0.001369\tcv-train-error:0.067476+0.000704\n"
     ]
    }
   ],
   "source": [
    "## get the list of features \n",
    "import xgboost as xgb\n",
    "d_train = xgb.DMatrix(train_svd, label=train_set.label)\n",
    "\n",
    "params = {}\n",
    "params['objective'] = 'binary:logistic'\n",
    "#params['objective'] = 'multi:softmax'\n",
    "#params['objective'] = 'multi:softprob'\n",
    "#params['eval_metric'] = 'merror'\n",
    "#params['booster']=\"gblinear\"\n",
    "params['eta'] = 0.1\n",
    "params['max_depth'] = 10\n",
    "params[\"subsample\"] = 0.5\n",
    "#params['num_class'] = 2 # Three Classes\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"colsample_bytree\"] = 0.5\n",
    "\n",
    "model = xgb.cv(dtrain=d_train,nfold=3,params=params,num_boost_round=20)\n",
    "model = xgb.train(dtrain=d_train,params=params,num_boost_round=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.93534258004\n",
      "0.148351648352\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "test_svd_tf = xgb.DMatrix(test_svd)\n",
    "xgb_pred=model.predict(test_svd_tf)\n",
    "xgb_pred_class = np.where(xgb_pred>0.3,1,0)\n",
    "print accuracy_score(y_true=test_set.label, y_pred=xgb_pred_class)\n",
    "print f1_score(y_true=test_set.label, y_pred=xgb_pred_class, average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.966106997601\n",
      "0.717636837533\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC, LinearSVC, NuSVC\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "clf = LinearSVC(C=4)\n",
    "clf.fit(train_tfidf,train_set.label)\n",
    "svm_pred_test_label=clf.predict(test_tfidf)\n",
    "\n",
    "print accuracy_score(y_true=test_set.label, y_pred=svm_pred_test_label)\n",
    "print f1_score(y_true=test_set.label, y_pred=svm_pred_test_label, average='macro')\n"
   ]
  },
  {