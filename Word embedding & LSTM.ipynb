
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word  Embedding: \n",
    "Word Embedding is representation words in a numerical format. Usually 100 to 500 numneric values considered to represent a word in word Embedding\n",
    "#### Example:\n",
    "##### sea : Vector1= [ 0.29919001 -0.11731    -0.0089925  -0.37059    -0.06722     0.15163  -0.061105    0.29587001  0.36515999 -1.50870001] ## 10 numeric values                                      \n",
    "#### ocean: Vector2 = [[ 0.12654001 -0.64278001 -0.45298001 -0.01482    -0.14442     0.16745999 -0.085392   -0.20653     0.58867002 -1.45519996]] ## 10 numeric values             \n",
    "#### car: Vector3= [[ 0.46443     0.37729999 -0.21459    -0.50768    -0.24575999  0.08134 0.10145     0.25154999 -0.36151999 -1.60300004]] ## 10 numeric values \n",
    "The similarity between Vecor1 & Vector2 are high compare to Vector1 and Vecror3    \n",
    "These word's vector can be used in multiple NLP taskes like finding similarity between two words and also can be useud in classification task\n",
    "\n",
    "### word2vec:\n",
    "It creates word to numeric vector by learning large text corpus using Deep Learning\n",
    "\n",
    "#### Use of Word Embedding in Keras to build LSTM Model\n",
    "\n",
    "#### Tokenizer\n",
    "Its Keras class in text preprocessing especially used for text tokenization and getting token level information using multiple methods\n",
    "#### Methods\n",
    "- fit_on_texts : It creates index to all tokens which can be seen using word_index property\n",
    "- texts_to_sequences : It maps the index to popular words to the given text from fit_on_texts\n",
    "\n",
    "### Use  Pre Trainied Word Embeddings in Modelling \n",
    "   - Loading the GloVe Word Embedding\n",
    "   - Create Embedding Matrix : It maps numeric vector each words received from tokenization\n",
    "   - Create Embedding Layer using values of Embedding Matrix ## It makes the matrix compatible\n",
    "   \n",
    "### Create X ( Text to be Classified) and y (Classification)\n",
    "- Convert text into sequence using text_sequence\n",
    "- pad the text sequence to get X\n",
    "- convert values y to array\n",
    "\n",
    "### Fit The model\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 5, 'lyft': 6, \"don't\": 20, 'when': 21, 'is': 2, 'unbelievable': 27, \"we'd\": 11, 'in': 3, 'need': 12, 'dysfunction': 13, 'selfish': 14, '21st': 15, 'for': 17, 'credit': 43, 'neverump': 18, \"it's\": 19, 'century': 7, 'father': 8, 'getthanked': 22, 'that': 29, 'dysfunctional': 23, 'disapointed': 32, 'cause': 25, 'into': 26, 'vans': 9, 'his': 4, 'run': 28, 'offer': 10, 'something': 47, 'use': 30, 'user': 1, 'they': 31, 'he': 33, 'a': 34, 'again': 35, 'kids': 36, 'like': 37, 'pdx': 39, 'i': 16, 'wheelchair': 41, 'xenophobia': 42, 'so': 44, 'this': 40, 'drags': 24, 'the': 45, \"can't\": 46, 'thanks': 38}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sample_text = [\"@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction. #run\",\n",
    "              \"it's unbelievable that in the 21st century we'd need something like this. again. #neverump #xenophobia\",\n",
    "              \"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx. #disapointed #getthanked\"]\n",
    "num_words = 100\n",
    "tokenizer = Tokenizer(nb_words=num_words) ## Tokenize the texts\n",
    "\n",
    "tokenizer.fit_on_texts(sample_text) ## Assiging inxed to all token\n",
    "\n",
    "print tokenizer.word_index ## See the index of all tokens \n",
    "#print \"#### #######\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import enchant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11, 12, 47, 37, 40, 35, 18, 42]]\n"
     ]
    }
   ],
   "source": [
    "print tokenizer.texts_to_sequences([\"we'd need something like this. again. #neverump #xenophobia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400000it [00:23, 17003.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#### Load the Pre Trained Glove Word Embedding \n",
    "\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.300d.txt')\n",
    "for line in tqdm(f):\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.          0.56586078]\n",
      " [ 0.56586078  1.        ]]\n"
     ]
    }
   ],
   "source": [
    "#embeddings_index\n",
    "import numpy as np\n",
    "#print embeddings_index['car']## Find the word vector\n",
    "print np.corrcoef(embeddings_index['car'],embeddings_index['bus'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [00:00<00:00, 214507.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47\n",
      "(48, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "## Create the word Embedding Matrix\n",
    "word_index = tokenizer.word_index  ### Number of words in the text sample\n",
    "print len(word_index)\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, 300)) ### use same dimension length of embedding vector\n",
    "for word, i in tqdm(word_index.items()):\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>290</th>\n",
       "      <th>291</th>\n",
       "      <th>292</th>\n",
       "      <th>293</th>\n",
       "      <th>294</th>\n",
       "      <th>295</th>\n",
       "      <th>296</th>\n",
       "      <th>297</th>\n",
       "      <th>298</th>\n",
       "      <th>299</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.116900</td>\n",
       "      <td>0.280150</td>\n",
       "      <td>0.129010</td>\n",
       "      <td>-0.857920</td>\n",
       "      <td>0.188440</td>\n",
       "      <td>0.007536</td>\n",
       "      <td>-0.295840</td>\n",
       "      <td>-0.149820</td>\n",
       "      <td>0.249070</td>\n",
       "      <td>-1.567700</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014962</td>\n",
       "      <td>-0.361060</td>\n",
       "      <td>-0.552190</td>\n",
       "      <td>-0.503630</td>\n",
       "      <td>0.302000</td>\n",
       "      <td>0.037021</td>\n",
       "      <td>-0.394720</td>\n",
       "      <td>0.341600</td>\n",
       "      <td>-0.142870</td>\n",
       "      <td>-0.449800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.174900</td>\n",
       "      <td>0.229560</td>\n",
       "      <td>0.249240</td>\n",
       "      <td>-0.205120</td>\n",
       "      <td>-0.122940</td>\n",
       "      <td>0.021297</td>\n",
       "      <td>-0.238150</td>\n",
       "      <td>0.137370</td>\n",
       "      <td>-0.089130</td>\n",