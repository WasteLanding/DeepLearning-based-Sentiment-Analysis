
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word  Embedding: \n",
    "Word Embedding is representation words in a numerical format. Usually 100 to 500 numneric values considered to represent a word in word Embedding\n",
    "#### Example:\n",
    "##### sea : Vector1= [ 0.29919001 -0.11731    -0.0089925  -0.37059    -0.06722     0.15163  -0.061105    0.29587001  0.36515999 -1.50870001] ## 10 numeric values                                      \n",
    "#### ocean: Vector2 = [[ 0.12654001 -0.64278001 -0.45298001 -0.01482    -0.14442     0.16745999 -0.085392   -0.20653     0.58867002 -1.45519996]] ## 10 numeric values             \n",
    "#### car: Vector3= [[ 0.46443     0.37729999 -0.21459    -0.50768    -0.24575999  0.08134 0.10145     0.25154999 -0.36151999 -1.60300004]] ## 10 numeric values \n",
    "The similarity between Vecor1 & Vector2 are high compare to Vector1 and Vecror3    \n",
    "These word's vector can be used in multiple NLP taskes like finding similarity between two words and also can be useud in classification task\n",
    "\n",
    "### word2vec:\n",
    "It creates word to numeric vector by learning large text corpus using Deep Learning\n",
    "\n",
    "#### Use of Word Embedding in Keras to build LSTM Model\n",
    "\n",
    "#### Tokenizer\n",
    "Its Keras class in text preprocessing especially used for text tokenization and getting token level information using multiple methods\n",
    "#### Methods\n",
    "- fit_on_texts : It creates index to all tokens which can be seen using word_index property\n",
    "- texts_to_sequences : It maps the index to popular words to the given text from fit_on_texts\n",
    "\n",
    "### Use  Pre Trainied Word Embeddings in Modelling \n",
    "   - Loading the GloVe Word Embedding\n",
    "   - Create Embedding Matrix : It maps numeric vector each words received from tokenization\n",
    "   - Create Embedding Layer using values of Embedding Matrix ## It makes the matrix compatible\n",
    "   \n",
    "### Create X ( Text to be Classified) and y (Classification)\n",
    "- Convert text into sequence using text_sequence\n",
    "- pad the text sequence to get X\n",
    "- convert values y to array\n",
    "\n",
    "### Fit The model\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "keras.preprocessing.text.Tokenizer(num_words=None,\n",
    "                                   filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n',\n",
    "                                   lower=True,\n",
    "                                   split=\" \",\n",
    "                                   char_level=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'and': 5, 'lyft': 6, \"don't\": 20, 'when': 21, 'is': 2, 'unbelievable': 27, \"we'd\": 11, 'in': 3, 'need': 12, 'dysfunction': 13, 'selfish': 14, '21st': 15, 'for': 17, 'credit': 43, 'neverump': 18, \"it's\": 19, 'century': 7, 'father': 8, 'getthanked': 22, 'that': 29, 'dysfunctional': 23, 'disapointed': 32, 'cause': 25, 'into': 26, 'vans': 9, 'his': 4, 'run': 28, 'offer': 10, 'something': 47, 'use': 30, 'user': 1, 'they': 31, 'he': 33, 'a': 34, 'again': 35, 'kids': 36, 'like': 37, 'pdx': 39, 'i': 16, 'wheelchair': 41, 'xenophobia': 42, 'so': 44, 'this': 40, 'drags': 24, 'the': 45, \"can't\": 46, 'thanks': 38}\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "sample_text = [\"@user when a father is dysfunctional and is so selfish he drags his kids into his dysfunction. #run\",\n",
    "              \"it's unbelievable that in the 21st century we'd need something like this. again. #neverump #xenophobia\",\n",
    "              \"@user @user thanks for #lyft credit i can't use cause they don't offer wheelchair vans in pdx. #disapointed #getthanked\"]\n",
    "num_words = 100\n",
    "tokenizer = Tokenizer(nb_words=num_words) ## Tokenize the texts\n",
    "\n",
    "tokenizer.fit_on_texts(sample_text) ## Assiging inxed to all token\n",
    "\n",
    "print tokenizer.word_index ## See the index of all tokens \n",
    "#print \"#### #######\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },