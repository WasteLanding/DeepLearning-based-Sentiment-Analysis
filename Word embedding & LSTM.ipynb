
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word  Embedding: \n",
    "Word Embedding is representation words in a numerical format. Usually 100 to 500 numneric values considered to represent a word in word Embedding\n",
    "#### Example:\n",
    "##### sea : Vector1= [ 0.29919001 -0.11731    -0.0089925  -0.37059    -0.06722     0.15163  -0.061105    0.29587001  0.36515999 -1.50870001] ## 10 numeric values                                      \n",
    "#### ocean: Vector2 = [[ 0.12654001 -0.64278001 -0.45298001 -0.01482    -0.14442     0.16745999 -0.085392   -0.20653     0.58867002 -1.45519996]] ## 10 numeric values             \n",
    "#### car: Vector3= [[ 0.46443     0.37729999 -0.21459    -0.50768    -0.24575999  0.08134 0.10145     0.25154999 -0.36151999 -1.60300004]] ## 10 numeric values \n",
    "The similarity between Vecor1 & Vector2 are high compare to Vector1 and Vecror3    \n",
    "These word's vector can be used in multiple NLP taskes like finding similarity between two words and also can be useud in classification task\n",
    "\n",
    "### word2vec:\n",
    "It creates word to numeric vector by learning large text corpus using Deep Learning\n",
    "\n",
    "#### Use of Word Embedding in Keras to build LSTM Model\n",
    "\n",
    "#### Tokenizer\n",
    "Its Keras class in text preprocessing especially used for text tokenization and getting token level information using multiple methods\n",
    "#### Methods\n",
    "- fit_on_texts : It creates index to all tokens which can be seen using word_index property\n",
    "- texts_to_sequences : It maps the index to popular words to the given text from fit_on_texts\n",
    "\n",